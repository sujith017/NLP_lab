{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9029af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "\n",
    "# Download necessary datasets\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load \"Alice in Wonderland\" text\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115900b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "sample_text = (\n",
    "    'We will discuss briefly about the basic syntax, structure and design philosophies. '\n",
    "    'There is a defined hierarchical syntax for Python code which you should remember '\n",
    "    'when writing code! Python is a really powerful programming language!'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b81f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default sentence tokenizer\n",
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a1ee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 3\n",
      "Sample text sentences:\n",
      "['We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should '\n",
      " 'remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print('Total sentences in sample_text:', len(sample_sentences))\n",
    "print('Sample text sentences:')\n",
    "pprint(sample_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38e5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.']\n"
     ]
    }
   ],
   "source": [
    "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
    "print('First 5 sentences in alice:')\n",
    "pprint(alice_sentences[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caee7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb675ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'you', 'know', 'that', 'the', 'U.S.', 'is', 'a', 'big', 'country', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Don't you know that the U.S. is a big country?\"\n",
    "words = word_tokenize(sentence)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b7eab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'you', 'know', 'that', 'the', 'U.S.', 'is', 'a', 'big', 'country', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "treebank_wt = TreebankWordTokenizer()\n",
    "sentence = \"Don't you know that the U.S. is a big country?\"\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a205c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'you', 'know', 'that', 'the', 'U', 'S', 'is', 'a', 'big', 'country']\n"
     ]
    }
   ],
   "source": [
    "# Regex word tokenizer\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "words = regex_wt.tokenize( \"Don't you know that the U.S. is a big country?\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ffa045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another sample sentence\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa8912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading necessary datasets for tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d89069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('quick', 'ADJ'), ('and', 'CONJ'), ('he', 'PRON'), ('is', 'VERB'), ('jumping', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Part-of-speech tagging\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba986bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building your own tagger\n",
    "from nltk.corpus import treebank\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53a31491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "612cac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_1844\\3532716376.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(dt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1454158195372253\n",
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NN'), ('jumping', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Default tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "print(dt.evaluate(test_data))\n",
    "print(dt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5453a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_1844\\3532716376.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(dt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1454158195372253\n",
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NN'), ('jumping', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Default tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "print(dt.evaluate(test_data))\n",
    "print(dt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdb84524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization\n",
    "import re\n",
    "import string\n",
    "\n",
    "corpus = [\n",
    "    \"The brown fox wasn't that quick and he couldn't win the race\",\n",
    "    \"Hey that's a great deal! I just bought a phone for $199\",\n",
    "    \"@@You'll (learn) a **lot** in the book. Python is an amazing language!@@\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d68dc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The',\n",
      "   'brown',\n",
      "   'fox',\n",
      "   'was',\n",
      "   \"n't\",\n",
      "   'that',\n",
      "   'quick',\n",
      "   'and',\n",
      "   'he',\n",
      "   'could',\n",
      "   \"n't\",\n",
      "   'win',\n",
      "   'the',\n",
      "   'race']],\n",
      " [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n",
      "  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n",
      " [['@',\n",
      "   '@',\n",
      "   'You',\n",
      "   \"'ll\",\n",
      "   '(',\n",
      "   'learn',\n",
      "   ')',\n",
      "   'a',\n",
      "   '*',\n",
      "   '*',\n",
      "   'lot',\n",
      "   '*',\n",
      "   '*',\n",
      "   'in',\n",
      "   'the',\n",
      "   'book',\n",
      "   '.'],\n",
      "  ['Python', 'is', 'an', 'amazing', 'language', '!'],\n",
      "  ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "token_list = [tokenize_text(text) for text in corpus]\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a50f6acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<filter object at 0x000002C32F007670>, <filter object at 0x000002C32F006F50>, <filter object at 0x000002C32F007A60>]\n"
     ]
    }
   ],
   "source": [
    "# Removing extra characters\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    return filtered_tokens\n",
    "\n",
    "filtered_list_1 = [\n",
    "    filter(None, [remove_characters_after_tokenization(tokens) for tokens in sentence_tokens])\n",
    "    for sentence_tokens in token_list\n",
    "]\n",
    "\n",
    "print(filtered_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a15a4add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race\n",
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case conversion\n",
    "print(corpus[0].lower())\n",
    "print(corpus[0].upper())\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41aebc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'brown', 'fox', 'nt', 'quick', 'could', 'nt', 'win', 'race'],\n",
      " ['Hey', 'great', 'deal', 'I', 'bought', 'phone', '199'],\n",
      " ['You', 'learn', 'lot', 'book', 'Python', 'amazing', 'language']]\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens\n",
    "\n",
    "cleaned_corpus_tokens = [\n",
    "    remove_stopwords([token for tokens in sentence_tokens for token in tokens])\n",
    "    for sentence_tokens in filtered_list_1\n",
    "]\n",
    "\n",
    "pprint(cleaned_corpus_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84316915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Obtaining dependency information for contractions from https://files.pythonhosted.org/packages/bb/e4/725241b788963b460ce0118bfd5c505dd3d1bdd020ee740f9f39044ed4a7/contractions-0.1.73-py2.py3-none-any.whl.metadata\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Obtaining dependency information for textsearch>=0.0.21 from https://files.pythonhosted.org/packages/e2/0f/6f08dd89e9d71380a369b1f5b6c97a32d62fc9cfacc1c5b8329505b9e495/textsearch-0.0.24-py2.py3-none-any.whl.metadata\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Obtaining dependency information for anyascii from https://files.pythonhosted.org/packages/4f/7b/a9a747e0632271d855da379532b05a62c58e979813814a57fa3b3afeb3a4/anyascii-0.3.2-py3-none-any.whl.metadata\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Obtaining dependency information for pyahocorasick from https://files.pythonhosted.org/packages/36/76/d83c60ec7a202cbfeffaa9649d0fee6ddcb974622e411b86211ff3572549/pyahocorasick-2.1.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyahocorasick-2.1.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "   ---------------------------------------- 0.0/289.9 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 92.2/289.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 122.9/289.9 kB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 174.1/289.9 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 204.8/289.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 256.0/289.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 289.9/289.9 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading pyahocorasick-2.1.0-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8430723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are going to do it, are not you?\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "# Example sentence with contractions\n",
    "sentence = \"You're going to do it, aren't you?\"\n",
    "\n",
    "# Expand contractions using the `fix` function\n",
    "expanded_sentence = contractions.fix(sentence)\n",
    "\n",
    "print(expanded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99adfe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Obtaining dependency information for textblob from https://files.pythonhosted.org/packages/02/07/5fd2945356dd839974d3a25de8a142dc37293c21315729a41e775b5f3569/textblob-0.18.0.post0-py3-none-any.whl.metadata\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/626.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/626.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/626.3 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/626.3 kB 163.8 kB/s eta 0:00:04\n",
      "   --- ----------------------------------- 61.4/626.3 kB 252.2 kB/s eta 0:00:03\n",
      "   ---- ---------------------------------- 71.7/626.3 kB 280.5 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 143.4/626.3 kB 448.2 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 143.4/626.3 kB 448.2 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 194.6/626.3 kB 471.4 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 204.8/626.3 kB 479.2 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 276.5/626.3 kB 548.9 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 317.4/626.3 kB 578.5 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 368.6/626.3 kB 603.4 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 419.8/626.3 kB 639.0 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 450.6/626.3 kB 655.2 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 481.3/626.3 kB 655.1 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 501.8/626.3 kB 616.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 553.0/626.3 kB 667.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 583.7/626.3 kB 655.4 kB/s eta 0:00:01\n",
      "   -------------------------------------  624.6/626.3 kB 655.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 626.3/626.3 kB 646.5 kB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88b325e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have a spelling mistake in this sentence.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Perform spell checking and correction\n",
    "    text = str(TextBlob(text).correct())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "sentence = \"I have a speling mistake in this sentense.\"\n",
    "normalized_sentence = normalize_text(sentence)\n",
    "\n",
    "print(normalized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbe03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
